{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accident Propensity Index Calculation v07\n",
    "--- Ready for Deployment ---\n",
    "\n",
    "Including efficiency features. Assumes that splines are not longer than around 5.5km since for each start point, only accidents within a radius of, in the most sub-optimal case, 5.5km are checked. This takes .08 seconds. When using 10km radius, it takes .17 seconds. With 100km radius it thakes .36 seconds.\n",
    "\n",
    "Imports json now and exports json with API calculated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import pandas as pd\n",
    "import math\n",
    "import folium\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficiency variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bucket size: 1 would be every longitude/latitude (ca. 110km); 10 is 1/10th (ca. 11km); 100 is 1/100th (ca. 1.1km); 1000 is 1/1000th (ca. 0.11km)\n",
    "bucket_length = 1000\n",
    "\n",
    "# Maximal distance of accidents from route in kilometers\n",
    "max_distance = 0.05"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split accident data into buckets - run once at start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataframe with the accident locations -> has only columns ID, Start_Lat, and Start_Lng\n",
    "raw_data = pd.read_csv('final_accident_data.csv')\n",
    "\n",
    "# Functions to generate latitude and longitude IDs for the bucketing\n",
    "def get_lat_id(lat):\n",
    "    return int(lat*bucket_length)\n",
    "def get_lng_id(lng):\n",
    "    return int(lng*bucket_length)\n",
    "\n",
    "# Assign bucketing IDs to each accident\n",
    "raw_data[\"lat_id\"] = raw_data[\"Start_Lat\"].apply(get_lat_id)\n",
    "raw_data[\"lng_id\"] = raw_data[\"Start_Lng\"].apply(get_lng_id)\n",
    "\n",
    "# Group by bucketing IDs\n",
    "groups = raw_data.groupby(['lat_id', 'lng_id'])\n",
    "\n",
    "# Iterate over the groups and create individual dataframes\n",
    "for name, group in groups:\n",
    "    # Create the dataframe name\n",
    "    df_name = f\"accidents_{name[0]}_{name[1]}\"\n",
    "    \n",
    "    # Create the dataframe\n",
    "    vars()[df_name] = group.copy()\n",
    "\n",
    "# Get a copy of all global variables\n",
    "global_vars = globals().copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to calculate distances and find accidents on route - run once at start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the distance between two points\n",
    "def distance(point1, point2):\n",
    "    lat1, lon1 = point1\n",
    "    lat2, lon2 = point2\n",
    "    km_per_lat = 110.574 # km per degree latitude\n",
    "    km_per_lon = 111.320 # km per degree longitude at the equator\n",
    "    dx = (lon2 - lon1) * km_per_lon * math.cos((lat1 + lat2) / 2)\n",
    "    dy = (lat2 - lat1) * km_per_lat\n",
    "    return math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "# Define a function to calculate the distance between a point and a line segment\n",
    "def distance_to_segment(point, segment_start, segment_end):\n",
    "    px, py = point\n",
    "    x1, y1 = segment_start\n",
    "    x2, y2 = segment_end\n",
    "    dx, dy = x2 - x1, y2 - y1\n",
    "    segment_length_squared = dx*dx + dy*dy\n",
    "    if segment_length_squared == 0:\n",
    "        return distance(point, segment_start)\n",
    "    t = max(0, min(1, ((px - x1) * dx + (py - y1) * dy) / segment_length_squared))\n",
    "    x = x1 + t * dx\n",
    "    y = y1 + t * dy\n",
    "    return distance(point, (x, y))\n",
    "\n",
    "# Generate dataframe of accidents close to the segment\n",
    "def generate_data(start_point):\n",
    "    # Extract the integer values of the start point lat and lng\n",
    "    start_lat = int(start_point[0]*bucket_length)\n",
    "    start_lng = int(start_point[1]*bucket_length)\n",
    "\n",
    "    # Get the dataframes that match the criteria\n",
    "    dfs_to_use = []\n",
    "    for lat_offset in [-1, 0, 1]:\n",
    "        for lng_offset in [-1, 0, 1]:\n",
    "            lat_id = start_lat + lat_offset\n",
    "            lng_id = start_lng + lng_offset\n",
    "            df_name = f\"accidents_{lat_id}_{lng_id}\"\n",
    "            if df_name in global_vars and isinstance(global_vars[df_name], pd.DataFrame):\n",
    "                dfs_to_use.append(global_vars[df_name])\n",
    "\n",
    "    # Concatenate the dataframes and reset the index\n",
    "    combined_df = pd.concat(dfs_to_use)\n",
    "    data = combined_df.reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "# Define a function to find accidents on a given route within a maximum distance\n",
    "def find_accidents_on_route(start_point, end_point):\n",
    "    # Create a mask for accidents that are within the maximum distance from the route\n",
    "    data = generate_data(start_point)\n",
    "    mask = data.apply(lambda row: distance_to_segment((row['Start_Lat'], row['Start_Lng']), start_point, end_point) <= max_distance, axis=1)\n",
    "\n",
    "    # Return the accidents that match the mask\n",
    "    accidents = data.loc[mask]\n",
    "    return accidents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find accidents on route - run every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from json file\n",
    "with open('route_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# create DataFrame from loaded data\n",
    "route_data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2z/5xl96y_10clcjhbp86qg3fyr0000gn/T/ipykernel_2387/2438444131.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  accidents_df = accidents_df.append(find_accidents_on_route(df.iloc[start_point], df.iloc[end_point]), ignore_index=True)\n",
      "/var/folders/2z/5xl96y_10clcjhbp86qg3fyr0000gn/T/ipykernel_2387/2438444131.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  accidents_df = accidents_df.append(find_accidents_on_route(df.iloc[start_point], df.iloc[end_point]), ignore_index=True)\n",
      "/var/folders/2z/5xl96y_10clcjhbp86qg3fyr0000gn/T/ipykernel_2387/2438444131.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  accidents_df = accidents_df.append(find_accidents_on_route(df.iloc[start_point], df.iloc[end_point]), ignore_index=True)\n",
      "/var/folders/2z/5xl96y_10clcjhbp86qg3fyr0000gn/T/ipykernel_2387/2438444131.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  accidents_df = accidents_df.append(find_accidents_on_route(df.iloc[start_point], df.iloc[end_point]), ignore_index=True)\n",
      "/var/folders/2z/5xl96y_10clcjhbp86qg3fyr0000gn/T/ipykernel_2387/2438444131.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  accidents_df = accidents_df.append(find_accidents_on_route(df.iloc[start_point], df.iloc[end_point]), ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Split the DataFrame into three equally sized parts\n",
    "df_list = np.array_split(route_df, 5)\n",
    "\n",
    "# Loop through the DataFrames\n",
    "for i, df in enumerate(df_list):\n",
    "    # Safe each split dataframe as a new route_df_{i} dataframe\n",
    "    globals()[f\"route_df_{i+1}\"] = df\n",
    "    # Create a new DataFrame to store the results\n",
    "    accidents_df = pd.DataFrame()\n",
    "    # Loop through the pairs of subsequent coordinates\n",
    "    for j in range(len(df) - 1):\n",
    "        start_point = j\n",
    "        end_point = j + 1\n",
    "        # Call the function and append the results to the accidents DataFrame\n",
    "        accidents_df = accidents_df.append(find_accidents_on_route(df.iloc[start_point], df.iloc[end_point]), ignore_index=True)\n",
    "    # Drop duplicate rows from the accidents DataFrame\n",
    "    accidents_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Assign a name to the accidents DataFrame based on the index of the original DataFrame\n",
    "    df_index = i+1\n",
    "    globals()[f'accidents_df_{df_index}'] = accidents_df\n",
    "    \n",
    "    # Print the resulting accidents DataFrame\n",
    "    # print(f'accidents_df_{df_index}:')\n",
    "    # print(accidents_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Accident Propensity Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of dataframes\n",
    "accidents_dfs = [accidents_df_1, accidents_df_2, accidents_df_3, accidents_df_4, accidents_df_5]\n",
    "api_dict = {}\n",
    "\n",
    "# loop through the list of dataframes and join each one with the all_accidents dataframe\n",
    "for i, df in enumerate(accidents_dfs, start=1):\n",
    "    exec(f'total_severity = accidents_df_{i}[\"Severity\"].sum()')\n",
    "    api = total_severity / 6035011\n",
    "    api_dict[f'api_{i}'] = round(api,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_dfs = [route_df_1, route_df_2, route_df_3, route_df_4, route_df_5]\n",
    "\n",
    "# Create a dictionary to store the JSON data\n",
    "json_dict = {}\n",
    "\n",
    "# Loop over the segments and add the API and route data to the JSON dictionary\n",
    "for i in range(5):\n",
    "    segment_name = f\"segment_{i+1}\"\n",
    "    json_dict[segment_name] = {}\n",
    "    json_dict[segment_name][\"api\"] = api_dict[f\"api_{i+1}\"]\n",
    "    json_dict[segment_name][\"route\"] = route_dfs[i].to_dict(orient=\"records\")\n",
    "\n",
    "# Write the JSON data to a file\n",
    "with open(\"backend_output.json\", \"w\") as f:\n",
    "    json.dump(json_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
